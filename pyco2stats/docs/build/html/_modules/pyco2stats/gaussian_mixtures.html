<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>pyco2stats.gaussian_mixtures &mdash; pyco2stats 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css" />

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            pyco2stats
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../modules.html">pyco2stats</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules.html#module-pyco2stats.gaussian_mixtures">pyco2stats Modules</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">pyco2stats</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Module code</a></li>
      <li class="breadcrumb-item active">pyco2stats.gaussian_mixtures</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for pyco2stats.gaussian_mixtures</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.exceptions</span> <span class="kn">import</span> <span class="n">ConvergenceWarning</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<div class="viewcode-block" id="GMM"><a class="viewcode-back" href="../../modules.html#pyco2stats.gaussian_mixtures.GMM">[docs]</a><span class="k">class</span> <span class="nc">GMM</span><span class="p">:</span>
<div class="viewcode-block" id="GMM.gaussian_mixture_em"><a class="viewcode-back" href="../../modules.html#pyco2stats.gaussian_mixtures.GMM.gaussian_mixture_em">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">gaussian_mixture_em</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n_components</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fit a Gaussian Mixture Model (GMM) to the given data using the Expectation-Maximization (EM) algorithm. From 10.1016/j.ijggc.2016.02.012</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        data : array-like</span>
<span class="sd">            The input data to fit the GMM to.</span>
<span class="sd">        n_components : int</span>
<span class="sd">            The number of Gaussian components in the mixture.</span>
<span class="sd">        max_iter : int</span>
<span class="sd">            The maximum number of iterations for the EM algorithm. Default is 100.</span>
<span class="sd">        tol : float</span>
<span class="sd">            The tolerance for convergence. Default is 1e-6.</span>

<span class="sd">        Returns:</span>
<span class="sd">        means : ndarray</span>
<span class="sd">            The means of the Gaussian components.</span>
<span class="sd">        std_devs : ndarray</span>
<span class="sd">            The standard deviations of the Gaussian components.</span>
<span class="sd">        weights : ndarray</span>
<span class="sd">            The weights (mixing proportions) of the Gaussian components.</span>
<span class="sd">        log_likelihoods : list</span>
<span class="sd">            The log-likelihood values over the iterations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>  <span class="c1"># Number of data points</span>

        <span class="c1"># Randomly initialize the parameters for the Gaussian components</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>  <span class="c1"># Seed for reproducibility</span>
        <span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n_components</span><span class="p">)</span>  <span class="c1"># Randomly pick initial means from the data</span>
        <span class="n">std_devs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n_components</span><span class="p">)</span>  <span class="c1"># Initialize standard deviations with random values</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_components</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_components</span>  <span class="c1"># Initialize weights uniformly</span>

        <span class="n">log_likelihoods</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># List to store the log-likelihood values over the iterations</span>
        
        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
            <span class="c1"># E-step: Compute the responsibilities (posterior probabilities) for each data point and component</span>
            <span class="n">responsibilities</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n_components</span><span class="p">))</span>  <span class="c1"># Initialize the responsibilities matrix</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components</span><span class="p">):</span>
                <span class="c1"># Calculate the responsibility of component k for each data point</span>
                <span class="n">responsibilities</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">means</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">std_devs</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
            
            <span class="c1"># Normalize responsibilities so that they sum to 1 for each data point</span>
            <span class="n">sum_responsibilities</span> <span class="o">=</span> <span class="n">responsibilities</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">responsibilities</span> <span class="o">/=</span> <span class="n">sum_responsibilities</span>
            
            <span class="c1"># M-step: Update the parameters (means, standard deviations, and weights) based on the responsibilities</span>
            <span class="n">N_k</span> <span class="o">=</span> <span class="n">responsibilities</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Effective number of points assigned to each component</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">N_k</span> <span class="o">/</span> <span class="n">n</span>  <span class="c1"># Update weights as the fraction of total points assigned to each component</span>
            
            <span class="c1"># Update means as the weighted average of the data points</span>
            <span class="n">means</span> <span class="o">=</span> <span class="p">(</span><span class="n">responsibilities</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">data</span><span class="p">)</span> <span class="o">/</span> <span class="n">N_k</span>
            
            <span class="c1"># Update standard deviations as the weighted standard deviation of the data points</span>
            <span class="n">std_devs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">responsibilities</span> <span class="o">*</span> <span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">-</span> <span class="n">means</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">N_k</span><span class="p">)</span>
            
            <span class="c1"># Compute the log-likelihood of the current parameter estimates</span>
            <span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sum_responsibilities</span><span class="p">))</span>
            <span class="n">log_likelihoods</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_likelihood</span><span class="p">)</span>
            
            <span class="c1"># Check for convergence: if the log-likelihood improvement is below the tolerance, stop the algorithm</span>
            <span class="k">if</span> <span class="n">iteration</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">log_likelihoods</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">log_likelihoods</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
                <span class="k">break</span>
        
        <span class="c1"># Return the optimized parameters and the log-likelihood history</span>
        <span class="k">return</span> <span class="n">means</span><span class="p">,</span> <span class="n">std_devs</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">log_likelihoods</span></div>

<div class="viewcode-block" id="GMM.gaussian_mixture_sklearn"><a class="viewcode-back" href="../../modules.html#pyco2stats.gaussian_mixtures.GMM.gaussian_mixture_sklearn">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">gaussian_mixture_sklearn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_components</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">tol</span> <span class="o">=</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="n">n_init</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">suppress_warnings</span><span class="o">=</span> <span class="kc">True</span>  <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Representation of a Gaussian mixture model probability distribution. Mutuated from sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array</span>

<span class="sd">        n_components : int</span>
<span class="sd">            The number of mixture components. Default is 3.</span>
<span class="sd">        max_iter : int</span>
<span class="sd">            The number of EM iterations to perform. Default is 10.</span>
<span class="sd">        tol : float</span>
<span class="sd">            The convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold. Default is 1e-10.</span>
<span class="sd">        n_init : int</span>
<span class="sd">            The number of initializations to perform. The best results are kept. Default is 20.</span>
<span class="sd">        suppress_warnings : bool</span>
<span class="sd">            Set to True if user wants to avoid the warnings. Default is True.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        original_means : ndarray</span>
<span class="sd">            The means of the Gaussian components.</span>
<span class="sd">        original_std_devs : ndarray</span>
<span class="sd">            The standard deviations of the Gaussian components.</span>
<span class="sd">        weights : ndarray</span>
<span class="sd">            The weights (mixing proportions) of the Gaussian components.</span>
<span class="sd">        max_iter : int</span>
<span class="sd">            The number of EM iterations to perform.</span>
<span class="sd">        log_likelihoods : list</span>
<span class="sd">            The log-likelihood values over the iterations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Standardize data to avoid numerical issues</span>
        <span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
        <span class="n">X_scaled</span> <span class="o">=</span>  <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># Parameters for GMM</span>
        <span class="n">n_components</span> <span class="o">=</span> <span class="mi">3</span>
        <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Further increase max_iter</span>
        <span class="n">tol</span> <span class="o">=</span> <span class="mf">1e-10</span>  <span class="c1"># Decrease tol for stricter convergence</span>
        <span class="n">n_init</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># Increase number of initializations for better convergence</span>

        <span class="c1"># Try different covariance types if needed</span>
        <span class="n">covariance_type</span> <span class="o">=</span> <span class="s1">&#39;spherical&#39;</span>  <span class="c1"># You can also try &#39;tied&#39;, &#39;diag&#39;, or &#39;spherical&#39;</span>


        <span class="c1"># Suppress ConvergenceWarning for clean output</span>
        <span class="k">if</span> <span class="n">suppress_warnings</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="n">ConvergenceWarning</span><span class="p">)</span>
              
                <span class="c1"># Fit GMM with the parameters</span>
                <span class="n">gmm</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="n">covariance_type</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> 
                                      <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="n">n_init</span><span class="p">,</span> <span class="n">init_params</span><span class="o">=</span><span class="s1">&#39;kmeans&#39;</span><span class="p">)</span>
                <span class="n">gmm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Fit GMM with the parameters</span>
                <span class="n">gmm</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="n">covariance_type</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> 
                                      <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="n">n_init</span><span class="p">,</span> <span class="n">init_params</span><span class="o">=</span><span class="s1">&#39;kmeans&#39;</span><span class="p">)</span>
                <span class="n">gmm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>


        <span class="c1"># Get the optimized parameters</span>
        <span class="n">means</span> <span class="o">=</span> <span class="n">gmm</span><span class="o">.</span><span class="n">means_</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="n">std_devs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">gmm</span><span class="o">.</span><span class="n">covariances_</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span> <span class="k">if</span> <span class="n">covariance_type</span> <span class="o">==</span> <span class="s1">&#39;full&#39;</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">gmm</span><span class="o">.</span><span class="n">covariances_</span><span class="p">)</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">gmm</span><span class="o">.</span><span class="n">weights_</span>

        <span class="c1"># Inverse transform the means to the original scale</span>
        <span class="n">original_means</span> <span class="o">=</span>  <span class="n">scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">gmm</span><span class="o">.</span><span class="n">means_</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="n">original_std_devs</span> <span class="o">=</span> <span class="n">std_devs</span>  <span class="o">*</span> <span class="n">scaler</span><span class="o">.</span><span class="n">scale_</span>

        <span class="c1"># Custom tracking of log-likelihood over iterations</span>
        <span class="n">log_likelihoods</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># For tracking, we simulate iterations manually</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">suppress_warnings</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="n">ConvergenceWarning</span><span class="p">)</span>
                    <span class="n">gmm_iter</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="n">covariance_type</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> 
                                               <span class="n">max_iter</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="n">n_init</span><span class="p">,</span> <span class="n">init_params</span><span class="o">=</span><span class="s1">&#39;kmeans&#39;</span><span class="p">)</span>
                    <span class="n">gmm_iter</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
                    <span class="n">log_likelihoods</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gmm_iter</span><span class="o">.</span><span class="n">lower_bound_</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">gmm_iter</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="n">covariance_type</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> 
                                               <span class="n">max_iter</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="n">n_init</span><span class="p">,</span> <span class="n">init_params</span><span class="o">=</span><span class="s1">&#39;kmeans&#39;</span><span class="p">)</span>
                <span class="n">gmm_iter</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
                <span class="n">log_likelihoods</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gmm_iter</span><span class="o">.</span><span class="n">lower_bound_</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">original_means</span><span class="p">,</span> <span class="n">original_std_devs</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span> <span class="n">log_likelihoods</span></div>

<div class="viewcode-block" id="GMM.constrained_gaussian_mixture"><a class="viewcode-back" href="../../modules.html#pyco2stats.gaussian_mixtures.GMM.constrained_gaussian_mixture">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">constrained_gaussian_mixture</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">mean_constraints</span><span class="p">,</span> <span class="n">std_constraints</span><span class="p">,</span> <span class="n">n_components</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Optimize a Gaussian Mixture Model (GMM) using PyTorch with specified constraints on means and standard deviations.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like</span>
<span class="sd">            Input data to fit the GMM.</span>
<span class="sd">        mean_constraints : list of tuples</span>
<span class="sd">            List of tuples specifying (min, max) constraints for each component&#39;s mean.</span>
<span class="sd">        std_constraints : list of tuples</span>
<span class="sd">            List of tuples specifying (min, max) constraints for each component&#39;s standard deviation.</span>
<span class="sd">        n_components : int</span>
<span class="sd">            Number of Gaussian components in the mixture.</span>
<span class="sd">        n_epochs : int</span>
<span class="sd">            Number of iterations for optimization. Default is 5000.</span>
<span class="sd">        lr : float</span>
<span class="sd">            Learning rate for the optimizer. Default is 0.001.</span>
<span class="sd">        verbose : bool</span>
<span class="sd">            If True, prints progress every 200 epochs. Default is True.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        optimized_means : ndarray</span>
<span class="sd">            Optimized means of the Gaussian components.</span>
<span class="sd">        optimized_stds : ndarray</span>
<span class="sd">            Optimized standard deviations of the Gaussian components.</span>
<span class="sd">        optimized_weights : ndarray</span>
<span class="sd">            Optimized weights (mixing proportions) of the Gaussian components.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Convert input data to a PyTorch tensor</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1"># Initialize the means, standard deviations, and weights with initial values</span>
        <span class="c1"># Initialize the means by sampling within the mean constraints</span>
        <span class="n">initial_means</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">mean_constraints</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">high</span><span class="o">=</span><span class="n">mean_constraints</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
                                      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components</span><span class="p">)],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Initialize the standard deviations by sampling within the std constraints</span>
        <span class="n">initial_stds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">std_constraints</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">high</span><span class="o">=</span><span class="n">std_constraints</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
                                     <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components</span><span class="p">)],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Initialize weights uniformly</span>
        <span class="n">initial_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span> <span class="o">/</span> <span class="n">n_components</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components</span><span class="p">)],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Define the optimizer</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">initial_means</span><span class="p">,</span> <span class="n">initial_stds</span><span class="p">,</span> <span class="n">initial_weights</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">apply_constraints</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">stds</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Apply constraints to ensure that the means and standard deviations stay within specified bounds,</span>
<span class="sd">            and that weights are positive and normalized.</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components</span><span class="p">):</span>
                    <span class="c1"># Clamp means and standard deviations to their respective constraints</span>
                    <span class="n">means</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="n">mean_constraints</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">mean_constraints</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
                    <span class="n">stds</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="n">std_constraints</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">std_constraints</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
                <span class="c1"># Ensure weights are positive and normalize them to sum to 1</span>
                <span class="n">weights</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">weights</span> <span class="o">/=</span> <span class="n">weights</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="c1"># Training loop</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="c1"># Compute the negative log-likelihood</span>
            <span class="n">log_likelihood</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">:</span>
                <span class="n">mixture_prob</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components</span><span class="p">):</span>
                    <span class="n">weight</span> <span class="o">=</span> <span class="n">initial_weights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
                    <span class="n">mean</span> <span class="o">=</span> <span class="n">initial_means</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
                    <span class="n">std</span> <span class="o">=</span> <span class="n">initial_stds</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
                    <span class="c1"># Compute the probability density function (PDF) for the Gaussian component</span>
                    <span class="n">mixture_prob</span> <span class="o">+=</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span>
                <span class="n">log_likelihood</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mixture_prob</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">)</span>  <span class="c1"># Add a small value to prevent log(0)</span>

            <span class="c1"># Normalize the negative log-likelihood</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_likelihood</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

            <span class="c1"># Backpropagation and optimization step</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="c1"># Apply constraints to the parameters</span>
            <span class="n">apply_constraints</span><span class="p">(</span><span class="n">initial_means</span><span class="p">,</span> <span class="n">initial_stds</span><span class="p">,</span> <span class="n">initial_weights</span><span class="p">)</span>

            <span class="c1"># Print progress every 200 epochs if verbose is True</span>
            <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">200</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

        <span class="c1"># Extract the optimized parameters</span>
        <span class="n">optimized_means</span> <span class="o">=</span> <span class="n">initial_means</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">optimized_stds</span> <span class="o">=</span> <span class="n">initial_stds</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">optimized_weights</span> <span class="o">=</span> <span class="n">initial_weights</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">optimized_means</span><span class="p">,</span> <span class="n">optimized_stds</span><span class="p">,</span> <span class="n">optimized_weights</span></div>

<div class="viewcode-block" id="GMM.gaussian_mixture_pdf"><a class="viewcode-back" href="../../modules.html#pyco2stats.gaussian_mixtures.GMM.gaussian_mixture_pdf">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">gaussian_mixture_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">meds</span><span class="p">,</span> <span class="n">stds</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the PDF of a Gaussian Mixture Model.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : array</span>
<span class="sd">            x values at which to compute the PDF.</span>
<span class="sd">        means : list or array</span>
<span class="sd">            means for each Gaussian component.</span>
<span class="sd">        stds : list or array</span>
<span class="sd">            standard deviations for each Gaussian component.</span>
<span class="sd">        weights : list or array</span>
<span class="sd">            weights (relative importance that must sum to 1) for each Gaussian component.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        pdf : array</span>
<span class="sd">            The computed PDF values for the Gaussian Mixture Model at each x.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        
        <span class="c1"># Initialize the PDF to zero</span>
        <span class="n">pdf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
        
        <span class="c1"># Compute the PDF for each Gaussian component and sum them up</span>
        <span class="k">for</span> <span class="n">med</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">meds</span><span class="p">,</span> <span class="n">stds</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
            <span class="n">pdf</span> <span class="o">+=</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">med</span><span class="p">,</span> <span class="n">std</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">pdf</span></div></div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>